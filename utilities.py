"""
╔══════════════════════════════════════════════════════════════════════════════╗
║                    CONSOLIDATED UTILITIES MODULE                              ║
║        Utility Functions for Sequence Processing and Data Export             ║
╚══════════════════════════════════════════════════════════════════════════════╝

MODULE: utilities.py
AUTHOR: Dr. Venkata Rajesh Yella
VERSION: 2024.1 - Consolidated
LICENSE: MIT

DESCRIPTION:
    Consolidated module containing all utility functions for sequence processing,
    pattern loading, data export, and validation. Combines functionality from
    utils, load_hsdb, load_regex_registry, motif_patterns, and canonicalize_motif.

MAIN FUNCTIONS:
    Sequence Processing:
    - parse_fasta(): Parse FASTA format sequences
    - gc_content(): Calculate GC content
    - reverse_complement(): Generate reverse complement
    - validate_sequence(): Validate DNA sequence

    Data Export:
    - export_to_csv(): Export to CSV format
    - export_to_bed(): Export to BED format
    - export_to_json(): Export to JSON format

    Pattern Loading:
    - load_hyperscan_db(): Load pre-compiled Hyperscan databases
    - load_regex_registry(): Load regex pattern registries
    - get_motif_patterns(): Get patterns for specific motif classes

    Statistics:
    - get_basic_stats(): Calculate sequence statistics
    - quality_check_motifs(): Validate motif quality
"""

def canonicalize_motif(m):
    """
    Canonicalize motif dictionary to standard format.
    Maintains backward compatibility with Normalized_Score but doesn't require it.
    """
    mapping = {
        'Actual Score': 'Actual_Score',
        'ActualScore': 'Actual_Score',
        'Score': 'Score',
        'Normalized Score': 'Normalized_Score',
        'Normalized_Score': 'Normalized_Score',
        'Class': 'Class',
        'Type': 'Class',
        'Subclass': 'Subclass',
        'Subtype': 'Subclass',
        'Start': 'Start',
        'End': 'End',
        'Length': 'Length',
        'Sequence_Name': 'Sequence_Name',
        'Motif': 'Motif'
    }
    out = {}
    for new_key in mapping.values():
        aliases = [k for k, v in mapping.items() if v == new_key]
        val = None
        for alias in aliases:
            if alias in m:
                val = m[alias]
                break
        out[new_key] = val
    if out.get('Start') and out.get('End') and not out.get('Length'):
        out['Length'] = int(out['End']) - int(out['Start'])
    out['Class'] = str(out.get('Class') or 'Unknown')
    out['Subclass'] = str(out.get('Subclass') or 'Other')
    out['Actual_Score'] = float(out.get('Actual_Score') or m.get('Actual_Score') or m.get('Score') or 0.0)
    out['Score'] = float(out.get('Score') or out['Actual_Score'])
    # Keep Normalized_Score for backward compatibility if it exists, but set to 0 if not present
    out['Normalized_Score'] = float(out.get('Normalized_Score') or m.get('Normalized_Score') or 0.0)
    out['Motif'] = m.get('Motif') or m.get('matched_seq') or ''
    out['Sequence_Name'] = out.get('Sequence_Name') or m.get('sequence_name') or ''
    return out

"""
Helper to load Hyperscan DB or compile from patterns when needed.

API:
    load_db_for_class(class_name: str, registry_dir: str) -> (db, id_to_pattern, id_to_score)
      - db: hyperscan.Database instance (or None if hyperscan not available)
      - id_to_pattern: dict mapping id -> pattern string (tenmer for 10-mer, regex for others)
      - id_to_score: dict mapping id -> score (float)
      
    Supports both:
      - 10-mer patterns (ZDNA, APhilic): patterns have 'tenmer' key
      - Regex patterns (G4, IMotif, etc.): patterns have 'pattern' key
"""

import os
import pickle
import logging

logger = logging.getLogger(__name__)

try:
    import hyperscan
    _HYPERSCAN_AVAILABLE = True
except Exception:
    hyperscan = None
    _HYPERSCAN_AVAILABLE = False


def _load_registry(registry_dir: str, class_name: str):
    pkl_path = os.path.join(registry_dir, f"{class_name}_registry.pkl")
    json_path = os.path.join(registry_dir, f"{class_name}_registry.json")
    if os.path.isfile(pkl_path):
        with open(pkl_path, "rb") as fh:
            return pickle.load(fh)
    if os.path.isfile(json_path):
        import json
        with open(json_path, "r") as fh:
            return json.load(fh)
    raise FileNotFoundError(f"No registry found for {class_name} in {registry_dir}")


def load_db_for_class(class_name: str, registry_dir: str = "registry"):
    """
    Returns (db, id_to_pattern, id_to_score).
    db is a hyperscan.Database instance (or None if hyperscan not available).
    id_to_pattern and id_to_score are dicts mapping integer id -> pattern/tenmer / score.
    
    Handles both 10-mer patterns (with 'tenmer' key) and regex patterns (with 'pattern' key).
    """
    reg = _load_registry(registry_dir, class_name)
    patterns = reg.get("patterns", [])
    
    # Handle both 10-mer registries (tenmer) and regex registries (pattern)
    id_to_pattern = {}
    for p in patterns:
        pattern_id = int(p["id"])
        # Try 'tenmer' first (for 10-mer patterns like ZDNA, APhilic)
        if "tenmer" in p:
            id_to_pattern[pattern_id] = p["tenmer"]
        # Fall back to 'pattern' (for regex patterns like G4, IMotif, etc.)
        elif "pattern" in p:
            id_to_pattern[pattern_id] = p["pattern"]
        else:
            logger.warning(f"Pattern {pattern_id} in {class_name} has neither 'tenmer' nor 'pattern' key")
    
    id_to_score = {int(p["id"]): float(p.get("score", 0.0)) for p in patterns}

    db = None
    hsdb_path = os.path.join(registry_dir, f"{class_name}.hsdb")
    if _HYPERSCAN_AVAILABLE:
        try:
            db = hyperscan.Database()
            if os.path.isfile(hsdb_path):
                # Many hyperscan Python bindings provide deserialize()
                with open(hsdb_path, "rb") as fh:
                    raw = fh.read()
                try:
                    db.deserialize(raw)
                    logger.info(f"Loaded serialized DB for {class_name} from {hsdb_path}")
                except Exception:
                    # Fallback: compile from patterns
                    expressions = [id_to_pattern[i].encode("ascii") for i in sorted(id_to_pattern.keys())]
                    ids = sorted(id_to_pattern.keys())
                    db.compile(expressions=expressions, ids=ids, elements=len(expressions))
                    logger.warning(f"deserialize() failed; compiled DB for {class_name} from patterns")
            else:
                # No serialized DB; compile from patterns
                expressions = [id_to_pattern[i].encode("ascii") for i in sorted(id_to_pattern.keys())]
                ids = sorted(id_to_pattern.keys())
                db.compile(expressions=expressions, ids=ids, elements=len(expressions))
                logger.info(f"Compiled DB for {class_name} from patterns in {registry_dir}")
        except Exception as e:
            logger.error(f"Hyperscan operations failed: {e}")
            db = None
    else:
        logger.debug("Hyperscan not installed; returning None DB (use pure-Python matcher).")

    return db, id_to_pattern, id_to_score
"""
Helper to load regex pattern registries and compile them with Hyperscan.

This module provides utilities for loading and using regex-based pattern registries
(CurvedDNA, G4, IMotif) similar to how the 10-mer registries work for A-philic and Z-DNA.

API:
    load_registry_for_class(class_name: str, registry_dir: str) 
        -> (db, id_to_pattern, id_to_subclass, id_to_score)
      - db: hyperscan.Database instance (or None if hyperscan not available)
      - id_to_pattern: dict mapping id -> regex pattern (string)
      - id_to_subclass: dict mapping id -> subclass name (string)
      - id_to_score: dict mapping id -> score (float)
"""

import os
import pickle
import json
import logging

logger = logging.getLogger(__name__)

try:
    import hyperscan
    _HYPERSCAN_AVAILABLE = True
except Exception:
    hyperscan = None
    _HYPERSCAN_AVAILABLE = False


def _load_registry(registry_dir: str, class_name: str):
    """Load registry # from .pkl or .json file."""
    pkl_path = os.path.join(registry_dir, f"{class_name}_registry.pkl")
    json_path = os.path.join(registry_dir, f"{class_name}_registry.json")
    
    if os.path.isfile(pkl_path):
        with open(pkl_path, "rb") as fh:
            return pickle.load(fh)
    if os.path.isfile(json_path):
        with open(json_path, "r") as fh:
            return json.load(fh)
    
    raise FileNotFoundError(f"No registry found for {class_name} in {registry_dir}")


def load_registry_for_class(class_name: str, registry_dir: str = "registry"):
    """
    Load regex pattern registry and compile with Hyperscan.
    
    Returns (db, id_to_pattern, id_to_subclass, id_to_score).
    - db is a hyperscan.Database instance (or None if hyperscan not available).
    - id_to_pattern: dict mapping integer id -> regex pattern string
    - id_to_subclass: dict mapping integer id -> subclass name
    - id_to_score: dict mapping integer id -> score value
    """
    # Load registry data
    reg = _load_registry(registry_dir, class_name)
    patterns = reg.get("patterns", [])
    
    # Extract mappings
    id_to_pattern = {int(p["id"]): p["pattern"] for p in patterns}
    id_to_subclass = {int(p["id"]): p.get("subclass", "unknown") for p in patterns}
    id_to_score = {int(p["id"]): float(p.get("score", 0.0)) for p in patterns}
    
    # Compile Hyperscan database if available
    db = None
    if _HYPERSCAN_AVAILABLE:
        try:
            # Try to load pre-compiled DB first
            hsdb_path = os.path.join(registry_dir, f"{class_name}.hsdb")
            if os.path.isfile(hsdb_path):
                try:
                    db = hyperscan.Database()
                    with open(hsdb_path, "rb") as fh:
                        raw = fh.read()
                    db.deserialize(raw)
                    logger.info(f"Loaded serialized Hyperscan DB for {class_name}")
                except Exception as e:
                    logger.warning(f"Failed to deserialize {class_name}.hsdb: {e}")
                    db = None
            
            # If no pre-compiled DB, compile from patterns
            if db is None:
                expressions = []
                ids = []
                flags = []
                
                for i in sorted(id_to_pattern.keys()):
                    expressions.append(id_to_pattern[i].encode("ascii"))
                    ids.append(i)
                    # Use CASELESS and DOTALL flags for DNA matching
                    flags.append(hyperscan.HS_FLAG_CASELESS | hyperscan.HS_FLAG_DOTALL)
                
                db = hyperscan.Database()
                db.compile(
                    expressions=expressions,
                    ids=ids,
                    elements=len(expressions),
                    flags=flags
                )
                logger.info(f"Compiled Hyperscan DB for {class_name} from {len(expressions)} patterns")
                
        except Exception as e:
            logger.error(f"Hyperscan compilation failed for {class_name}: {e}")
            db = None
    else:
        logger.debug(f"Hyperscan not available for {class_name}; use pure-Python matching")
    
    return db, id_to_pattern, id_to_subclass, id_to_score


# Cache compiled databases to avoid recompilation
_REGISTRY_CACHE = {}


def get_cached_registry(class_name: str, registry_dir: str = "registry"):
    """
    Get cached registry or load and compile it.
    Returns (db, id_to_pattern, id_to_subclass, id_to_score).
    """
    cache_key = f"{registry_dir}/{class_name}"
    
    if cache_key not in _REGISTRY_CACHE:
        _REGISTRY_CACHE[cache_key] = load_registry_for_class(class_name, registry_dir)
    
    return _REGISTRY_CACHE[cache_key]


def scan_with_registry(class_name: str, sequence: str, registry_dir: str = "registry"):
    """
    Scan a sequence using a registry's compiled Hyperscan DB.
    
    Returns list of (start, end, pattern_id, subclass) matches.
    Falls back to pure-Python regex matching if Hyperscan not available.
    """
    db, id_to_pattern, id_to_subclass, id_to_score = get_cached_registry(class_name, registry_dir)
    matches = []
    
    if db is not None and _HYPERSCAN_AVAILABLE:
        # Use Hyperscan for fast matching
        def on_match(pattern_id, start, end, flags, context):
            subclass = id_to_subclass.get(pattern_id, "unknown")
            matches.append((start, end, pattern_id, subclass))
        
        try:
            db.scan(sequence.encode(), match_event_handler=on_match)
        except Exception as e:
            logger.error(f"Hyperscan scan failed for {class_name}: {e}")
            return []
    else:
        # Fallback to pure-Python regex matching
        import re
        for pattern_id in sorted(id_to_pattern.keys()):
            pattern = id_to_pattern[pattern_id]
            subclass = id_to_subclass[pattern_id]
            
            try:
                compiled_pattern = re.compile(pattern, re.IGNORECASE)
                for match in compiled_pattern.finditer(sequence):
                    matches.append((match.start(), match.end(), pattern_id, subclass))
            except re.error as e:
                logger.warning(f"Invalid regex pattern {pattern_id} in {class_name}: {e}")
                continue
    
    # Sort by start position
    matches.sort(key=lambda x: x[0])
    return matches
"""
Non-B DNA Motif Patterns Registry
=================================

Comprehensive pattern library with scoring algorithms for all 11 motif classes.
Optimized for scientific accuracy and Hyperscan compatibility.

PATTERN CLASSIFICATION TABLE:
=============================
Class | Patterns | Subclasses | Scoring Method            | References
------|----------|------------|---------------------------|------------------
  1   |    15    |     2      | A-tract curvature         | Olson 1998
  2   |    37    |     2      | Repeat instability        | Wells 2005  
  3   |    89    |     1      | Palindrome stability      | Lilley 2000
  4   |    11    |     1      | R-loop formation          | Skourti 2019
  5   |    14    |     2      | Triplex potential         | Frank-Kamenetskii 1995
  6   |    16    |     7      | G4Hunter algorithm        | Bedrat 2016
  7   |    12    |     3      | i-motif pH stability      | Zeraati 2018
  8   |    13    |     2      | Z-DNA transition          | Ho 1986
  9   |     8    |     1      | A-philic propensity       | Gorin 1995
 10   |  Dynamic |  Dynamic   | Overlap analysis          | This work
 11   |  Dynamic |  Dynamic   | Clustering algorithm      | This work

Total: 207+ patterns across 22+ subclasses
"""

import os
import re
import numpy as np
from typing import Dict, List, Tuple, Any, Optional
from collections import defaultdict

# Try to import Hyperscan for high-performance pattern matching
try:
    import hyperscan
    HYPERSCAN_AVAILABLE = True
except (ImportError, Exception):
    HYPERSCAN_AVAILABLE = False

# =============================================================================
# COMPREHENSIVE PATTERN REGISTRY
# =============================================================================

class PatternRegistry:
    """Complete registry of all Non-B DNA patterns with metadata"""
    
    # Class 1: Curved DNA - A-tract mediated DNA bending (optimized)
    CURVED_DNA_PATTERNS = {
        'a_tracts': [
            # (pattern, pattern_id, name, subclass, min_len, scoring_method, confidence, biological_significance, reference)
            (r'A{4,15}', 'CRV_1_1', 'A-tract', 'Local Curvature', 4, 'curvature_score', 0.95, 'DNA bending', 'Crothers 1992'),
            (r'T{4,15}', 'CRV_1_2', 'T-tract', 'Local Curvature', 4, 'curvature_score', 0.95, 'DNA bending', 'Crothers 1992'),
            (r'(?:A{3,8}T{1,5}){2,}', 'CRV_1_3', 'AT-rich tract', 'Local Curvature', 8, 'curvature_score', 0.85, 'Sequence-dependent bending', 'Hagerman 1986'),
        ],
        'phased_a_tracts': [
            (r'(?:A{3,8}.{8,12}){3,}A{3,8}', 'CRV_1_4', 'Phased A-tracts', 'Global Curvature', 20, 'phasing_score', 0.90, 'Macroscopic curvature', 'Koo 1986'),
            (r'(?:T{3,8}.{8,12}){3,}T{3,8}', 'CRV_1_5', 'Phased T-tracts', 'Global Curvature', 20, 'phasing_score', 0.90, 'Macroscopic curvature', 'Koo 1986'),
        ]
    }
    
    # Class 2: Slipped DNA - Tandem repeat-induced slippage
    SLIPPED_DNA_PATTERNS = {
        'short_tandem_repeats': [
            (r'([ATGC])\1{9,}', 'SLP_2_1', 'Mononucleotide repeat', 'STR', 10, 'instability_score', 0.98, 'Replication slippage', 'Schlötterer 2000'),
            (r'([ATGC]{2})\1{4,}', 'SLP_2_2', 'Dinucleotide repeat', 'STR', 10, 'instability_score', 0.95, 'Microsatellite instability', 'Weber 1989'),
            (r'([ATGC]{3})\1{3,}', 'SLP_2_3', 'Trinucleotide repeat', 'STR', 12, 'instability_score', 0.92, 'Expansion diseases', 'Ashley 1993'),
            (r'([ATGC]{4})\1{2,}', 'SLP_2_4', 'Tetranucleotide repeat', 'STR', 12, 'instability_score', 0.85, 'Genetic polymorphisms', 'Edwards 1991'),
            (r'(CA)\1{4,}', 'SLP_2_5', 'CA repeat', 'STR', 10, 'instability_score', 0.95, 'Common microsatellite', 'Weber 1989'),
            (r'(CGG)\1{3,}', 'SLP_2_6', 'CGG repeat', 'STR', 12, 'instability_score', 0.90, 'Fragile X syndrome', 'Verkerk 1991'),
        ],
        'direct_repeats': [
            (r'([ATGC]{5,20})(?:[ATGC]{0,100})\1', 'SLP_2_7', 'Direct repeat', 'Direct Repeat', 10, 'repeat_score', 0.80, 'Recombination hotspots', 'Jeffreys 1985'),
            (r'([ATGC]{10,50})(?:[ATGC]{0,200})\1', 'SLP_2_8', 'Long direct repeat', 'Direct Repeat', 20, 'repeat_score', 0.75, 'Genomic instability', 'Lupski 1998'),
        ]
    }
    
    # Class 3: Cruciform DNA - Inverted repeat-induced four-way junctions  
    CRUCIFORM_PATTERNS = {
        'inverted_repeats': [
            (r'([ATGC]{6,20})[ATGC]{0,50}', 'CRU_3_1', 'Potential palindrome', 'Inverted Repeats', 12, 'cruciform_stability', 0.95, 'DNA secondary structure', 'Lilley 2000'),
            (r'([ATGC]{8,15})[ATGC]{2,20}([ATGC]{8,15})', 'CRU_3_2', 'Inverted repeat candidate', 'Inverted Repeats', 16, 'cruciform_stability', 0.80, 'Secondary structure prone', 'Pearson 1996'),
            (r'([ATGC]{4,10})[ATGC]{0,10}([ATGC]{4,10})', 'CRU_3_3', 'Short inverted repeat', 'Inverted Repeats', 8, 'cruciform_stability', 0.70, 'Local secondary structure', 'Sinden 1994'),
        ]
    }
    
    # Class 4: R-loop - RNA-DNA hybrid structures
    R_LOOP_PATTERNS = {
        'r_loop_formation_sites': [
            (r'[GC]{10,}[AT]{2,10}[GC]{10,}', 'RLP_4_1', 'GC-rich R-loop site', 'R-loop formation sites', 20, 'r_loop_potential', 0.85, 'Transcription-replication conflicts', 'Aguilera 2012'),
            (r'G{5,}[ATGC]{10,100}C{5,}', 'RLP_4_2', 'G-C rich region', 'R-loop formation sites', 20, 'r_loop_potential', 0.80, 'R-loop prone regions', 'Ginno 2012'),
            (r'[GC]{6,}[AT]{1,5}[GC]{6,}', 'RLP_4_3', 'GC-AT pattern', 'R-loop formation sites', 15, 'r_loop_potential', 0.75, 'Transcriptional pausing', 'Skourti-Stathaki 2011'),
        ],
        'qmrlfs_model_1': [
            (r'G{3,}[ATCGU]{1,10}?G{3,}(?:[ATCGU]{1,10}?G{3,}){1,}?', 'QmRLFS_4_1', 'QmRLFS Model 1', 'QmRLFS-m1', 25, 'qmrlfs_score', 0.90, 'RIZ detection with 3+ G tracts', 'Jenjaroenpun 2016'),
        ],
        'qmrlfs_model_2': [
            (r'G{4,}(?:[ATCGU]{1,10}?G{4,}){1,}?', 'QmRLFS_4_2', 'QmRLFS Model 2', 'QmRLFS-m2', 30, 'qmrlfs_score', 0.95, 'RIZ detection with 4+ G tracts', 'Jenjaroenpun 2016'),
        ]
    }
    
    # Class 5: Triplex DNA - Three-stranded structures (optimized)
    TRIPLEX_PATTERNS = {
        'triplex_forming_sequences': [
            (r'[GA]{15,}', 'TRX_5_1', 'Homopurine tract', 'Triplex', 15, 'triplex_potential', 0.90, 'H-DNA formation', 'Frank-Kamenetskii 1995'),
            (r'[CT]{15,}', 'TRX_5_2', 'Homopyrimidine tract', 'Triplex', 15, 'triplex_potential', 0.90, 'H-DNA formation', 'Frank-Kamenetskii 1995'),
            (r'(?:GA){6,}[GA]*(?:TC){6,}', 'TRX_5_3', 'Mirror repeat', 'Triplex', 24, 'triplex_potential', 0.85, 'Intermolecular triplex', 'Beal 1996'),
            (r'(?:GAA){4,}', 'TRX_5_4', 'GAA repeat', 'Sticky DNA', 12, 'sticky_dna_score', 0.95, 'Disease-associated repeats', 'Sakamoto 1999'),
            (r'(?:TTC){4,}', 'TRX_5_5', 'TTC repeat', 'Sticky DNA', 12, 'sticky_dna_score', 0.95, 'Disease-associated repeats', 'Sakamoto 1999'),
        ]
    }
    
    # Class 6: G-Quadruplex Family - Four-stranded G-rich structures (7 subclasses)
    G_QUADRUPLEX_PATTERNS = {
        'canonical_g4': [
            (r'G{3,}[ATGC]{1,7}G{3,}[ATGC]{1,7}G{3,}[ATGC]{1,7}G{3,}', 'G4_6_1', 'Canonical G4', 'Canonical G4', 15, 'g4hunter_score', 0.95, 'Stable G4 structures', 'Burge 2006'),
            (r'G{4,}[ATGC]{1,5}G{4,}[ATGC]{1,5}G{4,}[ATGC]{1,5}G{4,}', 'G4_6_2', 'High-density G4', 'Canonical G4', 16, 'g4hunter_score', 0.98, 'Very stable G4', 'Todd 2005'),
        ],
        'relaxed_g4': [
            (r'G{2,}[ATGC]{1,12}G{2,}[ATGC]{1,12}G{2,}[ATGC]{1,12}G{2,}', 'G4_6_3', 'Relaxed G4', 'Relaxed G4', 12, 'g4hunter_score', 0.80, 'Potential G4 structures', 'Huppert 2005'),
            (r'G{3,}[ATGC]{8,15}G{3,}[ATGC]{1,7}G{3,}[ATGC]{1,7}G{3,}', 'G4_6_4', 'Long-loop G4', 'Relaxed G4', 18, 'g4hunter_score', 0.75, 'Alternative G4 topology', 'Phan 2006'),
        ],
        'bulged_g4': [
            (r'G{3,}[ATGC]{8,25}G{3,}[ATGC]{1,7}G{3,}[ATGC]{1,7}G{3,}', 'G4_6_5', 'Bulged G4', 'Bulged G4', 20, 'g4hunter_score', 0.85, 'G4 with bulge loops', 'Lim 2009'),
            (r'G{2,}[ATGC]{15,40}G{2,}[ATGC]{1,7}G{2,}[ATGC]{1,7}G{2,}', 'G4_6_6', 'Large bulge G4', 'Bulged G4', 25, 'g4hunter_score', 0.70, 'Extended bulge G4', 'Adrian 2014'),
        ],
        'bipartite_g4': [
            (r'G{2,}[ATGC]{15,70}G{2,}[ATGC]{1,7}G{2,}[ATGC]{1,7}G{2,}', 'G4_6_7', 'Bipartite G4', 'Bipartite G4', 30, 'g4hunter_score', 0.75, 'Two-block G4 structures', 'Guédin 2010'),
        ],
        'multimeric_g4': [
            (r'(?:G{3,}[ATGC]{1,7}){4,}G{3,}', 'G4_6_8', 'Multimeric G4', 'Multimeric G4', 25, 'g4hunter_score', 0.90, 'Multiple G4 units', 'Phan 2007'),
            (r'(?:G{2,}[ATGC]{1,10}){5,}G{2,}', 'G4_6_9', 'Extended multimeric G4', 'Multimeric G4', 30, 'g4hunter_score', 0.85, 'Long G4 arrays', 'Maizels 2006'),
        ],
        'imperfect_g4': [
            (r'G{2,}[ATGC]{1,10}[AG]G{1,3}[ATGC]{1,10}G{2,}[ATGC]{1,10}G{2,}', 'G4_6_10', 'Imperfect G4', 'Imperfect G4', 15, 'g4hunter_score', 0.65, 'G4-like with interruptions', 'Kuryavyi 2010'),
            (r'G{3,}[ATGC]{1,7}[AG]{1,2}G{2,}[ATGC]{1,7}G{3,}[ATGC]{1,7}G{3,}', 'G4_6_11', 'G-rich imperfect', 'Imperfect G4', 18, 'g4hunter_score', 0.70, 'Interrupted G-tracts', 'Webba da Silva 2007'),
        ],
        'g_triplex': [
            (r'G{3,}[ATGC]{1,7}G{3,}[ATGC]{1,7}G{3,}', 'G4_6_12', 'G-Triplex', 'G-Triplex intermediate', 12, 'g_triplex_score', 0.80, 'Three G-tract structures', 'Sen 1988'),
            (r'G{4,}[ATGC]{1,5}G{4,}[ATGC]{1,5}G{4,}', 'G4_6_13', 'High-density G-triplex', 'G-Triplex intermediate', 14, 'g_triplex_score', 0.85, 'Stable three-tract G-structure', 'Williamson 1989'),
        ]
    }
    
    # Class 7: i-Motif Family - C-rich structures (3 subclasses)
    I_MOTIF_PATTERNS = {
        'canonical_imotif': [
            (r'C{3,}[ATGC]{1,7}C{3,}[ATGC]{1,7}C{3,}[ATGC]{1,7}C{3,}', 'IM_7_1', 'Canonical i-motif', 'Canonical i-motif', 15, 'imotif_score', 0.95, 'pH-dependent C-rich structure', 'Gehring 1993'),
            (r'C{4,}[ATGC]{1,5}C{4,}[ATGC]{1,5}C{4,}[ATGC]{1,5}C{4,}', 'IM_7_2', 'High-density i-motif', 'Canonical i-motif', 16, 'imotif_score', 0.98, 'Stable i-motif', 'Leroy 1993'),
        ],
        'relaxed_imotif': [
            (r'C{2,}[ATGC]{1,12}C{2,}[ATGC]{1,12}C{2,}[ATGC]{1,12}C{2,}', 'IM_7_3', 'Relaxed i-motif', 'Relaxed i-motif', 12, 'imotif_score', 0.80, 'Potential i-motif structures', 'Mergny 1995'),
            (r'C{3,}[ATGC]{8,15}C{3,}[ATGC]{1,7}C{3,}[ATGC]{1,7}C{3,}', 'IM_7_4', 'Long-loop i-motif', 'Relaxed i-motif', 18, 'imotif_score', 0.75, 'Alternative i-motif topology', 'Phan 2002'),
        ],
        'ac_motif': [
            (r'(?:AC){4,}|(?:CA){4,}', 'IM_7_5', 'AC-motif', 'AC-motif', 8, 'ac_motif_score', 0.85, 'AC alternating motif', 'Kang 2009'),
            (r'(?:A{2,3}C{2,3}){3,}', 'IM_7_6', 'Extended AC-motif', 'AC-motif', 12, 'ac_motif_score', 0.80, 'Variable AC motif', 'Zhou 2010'),
            (r'(?:ACG){3,}|(?:GCA){3,}', 'IM_7_7', 'ACG-motif', 'AC-motif', 9, 'ac_motif_score', 0.75, 'Trinucleotide AC motif', 'Liu 2012'),
        ]
    }
    
    # Class 8: Z-DNA - Left-handed double helix (2 subclasses)
    Z_DNA_PATTERNS = {
        'z_dna_canonical': [
            (r'(?:CG){4,}|(?:GC){4,}', 'ZDN_8_1', 'CG alternating', 'Z-DNA', 8, 'z_dna_score', 0.95, 'Classical Z-DNA sequence', 'Rich 1984'),
            (r'(?:CA){4,}(?:TG){4,}|(?:TG){4,}(?:CA){4,}', 'ZDN_8_2', 'CA-TG alternating', 'Z-DNA', 16, 'z_dna_score', 0.85, 'Alternative Z-DNA', 'Nordheim 1981'),
            (r'(?:AT){6,}|(?:TA){6,}', 'ZDN_8_3', 'AT alternating', 'Z-DNA', 12, 'z_dna_score', 0.75, 'AT-rich Z-DNA', 'Ellison 1986'),
        ],
        'egz_dna': [
            (r'[CG]{8,}', 'ZDN_8_4', 'CG-rich region', 'eGZ (Extruded-G) DNA', 8, 'egz_score', 0.90, 'Extruded-G Z-DNA', 'Herbert 1997'),
            (r'G{4,}C{4,}|C{4,}G{4,}', 'ZDN_8_5', 'GC clusters', 'eGZ (Extruded-G) DNA', 8, 'egz_score', 0.85, 'Clustered GC Z-DNA', 'Liu 1999'),
        ]
    }
    
    # Class 9: A-philic DNA - A-rich structural motifs
    A_PHILIC_PATTERNS = {
        'a_philic_tracts': [
            (r'A{6,}', 'APH_9_1', 'Poly-A tract', 'A-philic DNA', 6, 'a_philic_score', 0.95, 'A-rich structural element', 'Gorin 1995'),
            (r'(?:A{3,}[AT]){3,}', 'APH_9_2', 'A-rich region', 'A-philic DNA', 12, 'a_philic_score', 0.85, 'Mixed A-rich motif', 'Nelson 1987'),
            (r'(?:AA[AT]){4,}', 'APH_9_3', 'AAT/AAA motif', 'A-philic DNA', 12, 'a_philic_score', 0.80, 'Structured A-rich element', 'Crothers 1990'),
        ]
    }

    @classmethod
    def get_all_patterns(cls) -> Dict[str, Dict[str, List[Tuple]]]:
        """Return complete pattern registry"""
        return {
            'curved_dna': cls.CURVED_DNA_PATTERNS,
            'slipped_dna': cls.SLIPPED_DNA_PATTERNS,  
            'cruciform': cls.CRUCIFORM_PATTERNS,
            'r_loop': cls.R_LOOP_PATTERNS,
            'triplex': cls.TRIPLEX_PATTERNS,
            'g_quadruplex': cls.G_QUADRUPLEX_PATTERNS,
            'i_motif': cls.I_MOTIF_PATTERNS,
            'z_dna': cls.Z_DNA_PATTERNS,
            'a_philic': cls.A_PHILIC_PATTERNS
        }
    
    @classmethod
    def get_pattern_count(cls) -> Dict[str, int]:
        """Get pattern count statistics"""
        all_patterns = cls.get_all_patterns()
        counts = {}
        total = 0
        
        for motif_class, pattern_groups in all_patterns.items():
            class_count = sum(len(patterns) for patterns in pattern_groups.values())
            counts[motif_class] = class_count
            total += class_count
        
        counts['total'] = total
        return counts
    
    @classmethod 
    def get_subclass_mapping(cls) -> Dict[str, List[str]]:
        """Get mapping of classes to subclasses"""
        mapping = {
            'curved_dna': ['Global curvature', 'Local Curvature'],
            'slipped_dna': ['Direct Repeat', 'STR'],
            'cruciform': ['Inverted Repeats'],
            'r_loop': ['R-loop formation sites', 'QmRLFS-m1', 'QmRLFS-m2'],
            'triplex': ['Triplex', 'Sticky DNA'],
            'g_quadruplex': ['Multimeric G4', 'Canonical G4', 'Relaxed G4', 'Bulged G4', 
                           'Bipartite G4', 'Imperfect G4', 'G-Triplex intermediate'],
            'i_motif': ['Canonical i-motif', 'Relaxed i-motif', 'AC-motif'],
            'z_dna': ['Z-DNA', 'eGZ (Extruded-G) DNA'],
            'a_philic': ['A-philic DNA'],
            'hybrid': ['Dynamic overlaps'],
            'cluster': ['Dynamic clusters']
        }
        return mapping

# =============================================================================
# SCIENTIFIC SCORING ALGORITHMS
# =============================================================================

class MotifScoring:
    """Comprehensive scoring algorithms for all motif classes"""
    
    @staticmethod
    def g4hunter_score(sequence: str, window_size: int = 25) -> float:
        """
        G4Hunter algorithm for G-quadruplex scoring (Bedrat et al., 2016)
        
        Args:
            sequence: DNA sequence
            window_size: Sliding window size
            
        Returns:
            G4Hunter score (normalized)
        """
        if len(sequence) < window_size:
            window_size = len(sequence)
        
        scores = []
        for i in range(len(sequence) - window_size + 1):
            window = sequence[i:i + window_size]
            score = 0
            
            for base in window:
                if base == 'G':
                    score += 1
                elif base == 'C':
                    score -= 1
            
            scores.append(score)
        
        if not scores:
            return 0.0
        
        max_score = max(abs(s) for s in scores)
        return max_score / window_size
    
    @staticmethod
    def imotif_score(sequence: str) -> float:
        """
        i-motif scoring based on C-tract analysis (Zeraati et al., 2018)
        
        Args:
            sequence: DNA sequence
            
        Returns:
            i-motif formation score
        """
        if len(sequence) < 12:
            return 0.0
        
        # Count C-tracts
        c_tracts = re.findall(r'C{2,}', sequence)
        if len(c_tracts) < 3:
            return 0.0
        
        # Calculate score based on C-tract density and length
        total_c_length = sum(len(tract) for tract in c_tracts)
        c_density = total_c_length / len(sequence)
        tract_bonus = len(c_tracts) / 4  # Bonus for multiple tracts
        
        return min(c_density + tract_bonus * 0.2, 1.0)
    
    @staticmethod
    def z_dna_score(sequence: str) -> float:
        """
        Z-DNA scoring based on alternating purine-pyrimidine content (Ho et al., 1986)
        
        Args:
            sequence: DNA sequence
            
        Returns:
            Z-DNA formation probability
        """
        if len(sequence) < 6:
            return 0.0
        
        alternating_score = 0
        total_pairs = 0
        
        for i in range(len(sequence) - 1):
            curr_base = sequence[i]
            next_base = sequence[i + 1]
            total_pairs += 1
            
            # Score alternating purine-pyrimidine pattern
            if ((curr_base in 'AG' and next_base in 'CT') or 
                (curr_base in 'CT' and next_base in 'AG')):
                alternating_score += 1
                
                # Bonus for CG steps (classical Z-DNA)
                if (curr_base == 'C' and next_base == 'G') or (curr_base == 'G' and next_base == 'C'):
                    alternating_score += 0.5
        
        return alternating_score / total_pairs if total_pairs > 0 else 0.0
    
    @staticmethod
    def curvature_score(sequence: str) -> float:
        """
        DNA curvature scoring based on A-tract analysis (Olson et al., 1998)
        
        Args:
            sequence: DNA sequence
            
        Returns:
            Intrinsic curvature score
        """
        if len(sequence) < 4:
            return 0.0
        
        # Find A/T tracts
        a_tracts = re.findall(r'A{3,}', sequence)
        t_tracts = re.findall(r'T{3,}', sequence)
        
        # Calculate curvature based on tract length and frequency
        curvature = 0
        for tract in a_tracts + t_tracts:
            curvature += len(tract) ** 1.5  # Non-linear length dependency
        
        return min(curvature / len(sequence), 1.0)
    
    @staticmethod
    def triplex_potential(sequence: str) -> float:
        """
        Triplex formation potential (Frank-Kamenetskii & Mirkin, 1995)
        
        Args:
            sequence: DNA sequence
            
        Returns:
            Triplex formation score
        """
        if len(sequence) < 15:
            return 0.0
        
        # Calculate homopurine and homopyrimidine content
        purine_runs = re.findall(r'[AG]{5,}', sequence)
        pyrimidine_runs = re.findall(r'[CT]{5,}', sequence)
        
        # Score based on run lengths and purity
        purine_score = sum(len(run) ** 1.2 for run in purine_runs)
        pyrimidine_score = sum(len(run) ** 1.2 for run in pyrimidine_runs)
        
        max_score = max(purine_score, pyrimidine_score)
        return min(max_score / (len(sequence) ** 1.2), 1.0)
    
    @staticmethod
    def r_loop_potential(sequence: str) -> float:
        """
        R-loop formation potential (Aguilera & García-Muse, 2012)
        
        Args:
            sequence: DNA sequence
            
        Returns:
            R-loop formation score
        """
        if len(sequence) < 20:
            return 0.0
        
        # GC skew calculation
        gc_skew = 0
        for base in sequence:
            if base == 'G':
                gc_skew += 1
            elif base == 'C':
                gc_skew -= 1
        
        # Normalize by sequence length
        gc_skew = abs(gc_skew) / len(sequence)
        
        # GC content (R-loops prefer GC-rich regions)
        gc_content = len(re.findall(r'[GC]', sequence)) / len(sequence)
        
        return min(gc_skew * 0.6 + gc_content * 0.4, 1.0)
    
    @staticmethod
    def qmrlfs_score(sequence: str) -> float:
        """
        QmRLFS-based R-loop formation scoring (Jenjaroenpun & Wongsurawat, 2016)
        
        Implements the QmRLFS algorithm for R-loop forming sequence detection
        based on RIZ (R-loop Initiating Zone) and REZ (R-loop Extending Zone)
        
        Args:
            sequence: DNA sequence
            
        Returns:
            QmRLFS score (0.0-1.0)
        """
        try:
            from qmrlfs_finder import QmRLFSDetector
            
            if len(sequence) < 20:
                return 0.0
            
            # Use quick mode for scoring to avoid performance issues
            detector = QmRLFSDetector(quick_mode=True)
            results = detector.analyze_sequence(sequence, analyze_both_strands=False)
            
            if not results:
                return 0.0
            
            # Return the highest scoring RLFS
            max_score = max(result["qmrlfs_score"] for result in results)
            return max_score
            
        except ImportError:
            # Fallback to simple scoring if QmRLFS module not available
            return MotifScoring.r_loop_potential(sequence)
    
    @staticmethod
    def instability_score(sequence: str) -> float:
        """
        Repeat instability scoring for slipped DNA structures
        
        Args:
            sequence: DNA sequence
            
        Returns:
            Instability score based on repeat characteristics
        """
        if len(sequence) < 6:
            return 0.0
        
        # Find repeating units
        max_instability = 0
        
        for unit_length in range(1, min(7, len(sequence) // 3)):
            for i in range(len(sequence) - unit_length * 2):
                unit = sequence[i:i + unit_length]
                count = 1
                
                # Count consecutive repeats
                pos = i + unit_length
                while pos + unit_length <= len(sequence) and sequence[pos:pos + unit_length] == unit:
                    count += 1
                    pos += unit_length
                
                if count >= 3:  # At least 3 repeats
                    instability = count * (unit_length ** 0.5)
                    max_instability = max(max_instability, instability)
        
        return min(max_instability / 10, 1.0)
    
    @staticmethod
    def cruciform_stability(sequence: str) -> float:
        """
        Cruciform stability based on palindrome characteristics
        
        Args:
            sequence: DNA sequence
            
        Returns:
            Stability score for cruciform formation
        """
        if len(sequence) < 8:
            return 0.0
        
        def reverse_complement(seq):
            complement = {'A': 'T', 'T': 'A', 'G': 'C', 'C': 'G'}
            return ''.join(complement.get(base, base) for base in reversed(seq))
        
        max_palindrome = 0
        
        # Look for palindromic regions
        for i in range(len(sequence)):
            for j in range(i + 8, len(sequence) + 1):
                subseq = sequence[i:j]
                if subseq == reverse_complement(subseq):
                    palindrome_length = len(subseq)
                    stability = palindrome_length ** 1.5 / len(sequence)
                    max_palindrome = max(max_palindrome, stability)
        
        return min(max_palindrome, 1.0)
    
    @staticmethod
    def a_philic_score(sequence: str) -> float:
        """
        A-philic DNA scoring based on A-tract characteristics
        
        Args:
            sequence: DNA sequence
            
        Returns:
            A-philic propensity score
        """
        if len(sequence) < 6:
            return 0.0
        
        a_content = len(re.findall(r'A', sequence)) / len(sequence)
        a_tracts = re.findall(r'A{3,}', sequence)
        
        # Score based on A content and tract formation
        tract_bonus = sum(len(tract) ** 1.2 for tract in a_tracts) / len(sequence)
        
        return min(a_content * 0.7 + tract_bonus * 0.3, 1.0)

# =============================================================================
# HYPERSCAN INTEGRATION & OPTIMIZATION
# =============================================================================

class HyperscanManager:
    """Hyperscan database management for high-performance pattern matching"""
    
    def __init__(self):
        self.compiled_db = None
        self.pattern_info = {}
        self.hyperscan_available = HYPERSCAN_AVAILABLE
    
    def compile_database(self, patterns: List[Tuple[str, str]]) -> bool:
        """
        Compile Hyperscan database from patterns
        
        Args:
            patterns: List of (pattern, identifier) tuples
            
        Returns:
            True if compilation successful
        """
        if not self.hyperscan_available:
            return False
        
        try:
            # Prepare patterns for Hyperscan
            hyperscan_patterns = []
            for i, (pattern, pattern_id) in enumerate(patterns):
                hyperscan_patterns.append((pattern.encode(), i, hyperscan.HS_FLAG_CASELESS))
                self.pattern_info[i] = pattern_id
            
            # Compile database
            self.compiled_db = hyperscan.hs_compile_multi(hyperscan_patterns)
            return True
            
        except Exception as e:
            print(f"Hyperscan compilation failed: {e}")
            return False
    
    def scan_sequence(self, sequence: str) -> List[Tuple[int, int, str]]:
        """
        Scan sequence with compiled Hyperscan database
        
        Args:
            sequence: DNA sequence to scan
            
        Returns:
            List of (start, end, pattern_id) matches
        """
        if not self.compiled_db:
            return []
        
        matches = []
        
        def match_handler(pattern_id: int, start: int, end: int, flags: int, context=None):
            pattern_info = self.pattern_info.get(pattern_id, f'pattern_{pattern_id}')
            matches.append((start, end, pattern_info))
        
        try:
            hyperscan.hs_scan(self.compiled_db, sequence.encode(), match_handler, None)
        except Exception as e:
            print(f"Hyperscan scanning failed: {e}")
        
        return matches

# =============================================================================
# PATTERN VALIDATION & TESTING
# =============================================================================

def validate_all_patterns() -> Dict[str, Any]:
    """
    Validate all patterns for correctness and Hyperscan compatibility
    
    Returns:
        Validation results dictionary
    """
    results = {
        'total_patterns': 0,
        'valid_patterns': 0,
        'invalid_patterns': [],
        'hyperscan_compatible': 0,
        'pattern_counts': {},
        'validation_passed': True
    }
    
    all_patterns = PatternRegistry.get_all_patterns()
    
    for motif_class, pattern_groups in all_patterns.items():
        class_count = 0
        for pattern_group, patterns in pattern_groups.items():
            for pattern_tuple in patterns:
                if len(pattern_tuple) >= 9:  # Full tuple
                    pattern = pattern_tuple[0]
                    pattern_id = pattern_tuple[1]
                    
                    results['total_patterns'] += 1
                    class_count += 1
                    
                    # Test regex compilation
                    try:
                        re.compile(pattern, re.IGNORECASE)
                        results['valid_patterns'] += 1
                        
                        # Test Hyperscan compatibility
                        if not any(incompatible in pattern for incompatible in ['\\b', '\\B', '(?=', '(?!', '(?<=', '(?<!', '\\1', '\\2']):
                            results['hyperscan_compatible'] += 1
                        
                    except re.error as e:
                        results['invalid_patterns'].append((motif_class, pattern_id, str(e)))
                        results['validation_passed'] = False
        
        results['pattern_counts'][motif_class] = class_count
    
    return results

def run_pattern_tests() -> bool:
    """Run comprehensive pattern tests"""
    print("Running Non-B DNA Pattern Validation...")
    
    # Test sequences for each class
    test_sequences = {
        'curved_dna': 'AAAAAAAATTTTTTTAAAAATTTT',  # A/T tracts
        'slipped_dna': 'CACACACACACACACACACA',     # CA repeats
        'cruciform': 'ATGCATGCATGCATGC',          # Palindrome
        'r_loop': 'GGGCCCGGGATGCCCGGG',           # GC-rich R-loop site
        'triplex': 'GAAAGAAAGAAAGAAAGAAA',        # Homopurine tract
        'g_quadruplex': 'GGGTTAGGGTTAGGGTTAGGG',   # Canonical G4
        'i_motif': 'CCCTAACCCTAACCCTAACCC',       # Canonical i-motif
        'z_dna': 'CGCGCGCGCGCGCGCGCG',            # CG alternating
        'a_philic': 'AAAAAAAAAAAAAAAAA'           # Poly-A
    }
    
    validation_results = validate_all_patterns()
    print(f"Pattern validation: {validation_results['valid_patterns']}/{validation_results['total_patterns']} patterns valid")
    print(f"Hyperscan compatible: {validation_results['hyperscan_compatible']}/{validation_results['total_patterns']} patterns")
    
    # Test pattern matching
    all_patterns = PatternRegistry.get_all_patterns()
    scoring = MotifScoring()
    
    test_results = {}
    for motif_class, test_seq in test_sequences.items():
        if motif_class in all_patterns:
            matches_found = 0
            
            for pattern_group, patterns in all_patterns[motif_class].items():
                for pattern_tuple in patterns:
                    if len(pattern_tuple) >= 3:
                        pattern = pattern_tuple[0]
                        try:
                            compiled_pattern = re.compile(pattern, re.IGNORECASE)
                            if compiled_pattern.search(test_seq):
                                matches_found += 1
                                break
                        except:
                            continue
                            
                if matches_found > 0:
                    break
            
            test_results[motif_class] = matches_found > 0
            print(f"✓ {motif_class}: {'PASS' if matches_found > 0 else 'FAIL'}")
    
    all_passed = all(test_results.values()) and validation_results['validation_passed']
    print(f"\nOverall validation: {'PASSED' if all_passed else 'FAILED'}")
    
    return all_passed

# =============================================================================
# HYPERSCAN REGISTRY LOADER INTEGRATION
# =============================================================================

try:
    # from .load_hsdb import load_db_for_class
    _LOAD_HSDB_AVAILABLE = True
except ImportError:
    try:
        # Fallback: direct import
        import sys
        import os as _os
        sys.path.insert(0, _os.path.dirname(__file__))
        from load_hsdb import load_db_for_class
        _LOAD_HSDB_AVAILABLE = True
    except ImportError:
        _LOAD_HSDB_AVAILABLE = False
        load_db_for_class = None

# In-memory cache to avoid repeated compiles/deserializes
_HS_DB_CACHE = {}
_REGISTRY_CACHE = {}


def get_pattern_registry(class_name: str, registry_dir: str = "registry"):
    """
    Returns parsed registry dict (as saved by generator): contains 'patterns' list etc.
    Caches the result.
    """
    if not _LOAD_HSDB_AVAILABLE:
        raise ImportError("load_hsdb module not available")
    
    key = f"{registry_dir}/{class_name}"
    if key in _REGISTRY_CACHE:
        return _REGISTRY_CACHE[key]
    
    # Read registry directly from file
    import pickle
    import json
    pkl_path = os.path.join(registry_dir, f"{class_name}_registry.pkl")
    json_path = os.path.join(registry_dir, f"{class_name}_registry.json")
    
    if os.path.isfile(pkl_path):
        with open(pkl_path, "rb") as fh:
            full = pickle.load(fh)
    elif os.path.isfile(json_path):
        with open(json_path, "r") as fh:
            full = json.load(fh)
    else:
        raise FileNotFoundError(f"No registry found for {class_name} in {registry_dir}")
    
    _REGISTRY_CACHE[key] = full
    return full


def get_hs_db_for_class(class_name: str, registry_dir: str = "registry"):
    """
    Return (db, id_to_pattern, id_to_score). Caches DB in process memory.
    db is a hyperscan.Database instance (or None if hyperscan not available).
    id_to_pattern: dict mapping id -> pattern string (tenmer for 10-mer, regex for others)
    """
    if not _LOAD_HSDB_AVAILABLE:
        raise ImportError("load_hsdb module not available")
    
    key = f"{registry_dir}/{class_name}"
    if key in _HS_DB_CACHE:
        return _HS_DB_CACHE[key]
    
    db, id_to_pattern, id_to_score = load_db_for_class(class_name, registry_dir)
    _HS_DB_CACHE[key] = (db, id_to_pattern, id_to_score)
    return db, id_to_pattern, id_to_score


# =============================================================================
# PATTERN STATISTICS & INFORMATION
# =============================================================================

def get_pattern_statistics() -> Dict[str, Any]:
    """Get comprehensive pattern statistics"""
    counts = PatternRegistry.get_pattern_count()
    subclasses = PatternRegistry.get_subclass_mapping()
    
    stats = {
        'total_patterns': counts['total'],
        'total_classes': 11,
        'total_subclasses': sum(len(subs) for subs in subclasses.values()),
        'class_breakdown': {k: v for k, v in counts.items() if k != 'total'},
        'subclass_breakdown': subclasses,
        'scoring_methods': [
            'g4hunter_score', 'imotif_score', 'z_dna_score', 'curvature_score',
            'triplex_potential', 'r_loop_potential', 'qmrlfs_score', 'instability_score', 
            'cruciform_stability', 'a_philic_score'
        ]
    }
    
    return stats

if __name__ == "__main__":
    # Run validation and display statistics
    print("="*60)
    print("NON-B DNA MOTIF PATTERNS REGISTRY")
    print("="*60)
    
    success = run_pattern_tests()
    
    if success:
        stats = get_pattern_statistics()
        print(f"\nPattern Registry Statistics:")
        print(f"Total Patterns: {stats['total_patterns']}")
        print(f"Total Classes: {stats['total_classes']}")
        print(f"Total Subclasses: {stats['total_subclasses']}")
        
        print("\nClass Breakdown:")
        for class_name, count in stats['class_breakdown'].items():
            print(f"  {class_name:<15}: {count:>3} patterns")
        
        print("\nScoring Methods Available:")
        for method in stats['scoring_methods']:
            print(f"  • {method}")
            
    print("="*60)


# ============================================================================
# NBDScanner Utilities - Helper Functions & I/O Operations
# ============================================================================
"""
NBDScanner Utilities - Helper Functions & I/O Operations
========================================================

Consolidated utility functions for sequence processing, I/O operations,
statistical calculations, and data formatting for the NBDScanner system.

UTILITY FUNCTIONS TABLE:
========================
Category          | Functions                    | Description
------------------|------------------------------|----------------------------------
Sequence I/O      | parse_fasta, write_fasta    | FASTA file handling
Sequence Utils    | reverse_complement, gc_content | Basic sequence operations  
Statistics        | get_basic_stats, motif_stats | Sequence and motif statistics
Formatting        | wrap, format_results        | Output formatting
Validation        | validate_sequence, quality_check | Input validation
Export            | to_bed, to_csv, to_json     | Data export formats

Author: Dr. Venkata Rajesh Yella
License: MIT
Version: 2024.1
"""

import re
import os
import json
import csv
import pandas as pd
import numpy as np
from typing import Dict, List, Any, Optional, Union, Tuple
from collections import Counter, defaultdict
from io import StringIO
import warnings
warnings.filterwarnings("ignore")

# =============================================================================
# SEQUENCE I/O OPERATIONS
# =============================================================================

def parse_fasta(fasta_content: str) -> Dict[str, str]:
    """
    Parse FASTA format content into sequences dictionary
    
    Args:
        fasta_content: FASTA format string content
        
    Returns:
        Dictionary of {sequence_name: sequence}
    """
    sequences = {}
    current_name = None
    current_seq = []
    
    lines = fasta_content.strip().split('\n')
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
            
        if line.startswith('>'):
            # Save previous sequence
            if current_name and current_seq:
                sequences[current_name] = ''.join(current_seq)
            
            # Start new sequence
            current_name = line[1:].strip()
            if not current_name:
                current_name = f"sequence_{len(sequences) + 1}"
            current_seq = []
        else:
            # Add to current sequence
            current_seq.append(line.upper())
    
    # Save last sequence
    if current_name and current_seq:
        sequences[current_name] = ''.join(current_seq)
    
    return sequences

def write_fasta(sequences: Dict[str, str], filename: str) -> bool:
    """
    Write sequences to FASTA format file
    
    Args:
        sequences: Dictionary of {name: sequence}
        filename: Output filename
        
    Returns:
        True if successful
    """
    try:
        with open(filename, 'w') as f:
            for name, seq in sequences.items():
                f.write(f">{name}\n")
                # Write sequence in 80-character lines
                for i in range(0, len(seq), 80):
                    f.write(f"{seq[i:i+80]}\n")
        return True
    except Exception as e:
        print(f"Error writing FASTA file {filename}: {e}")
        return False

def read_fasta_file(filename: str) -> Dict[str, str]:
    """
    Read FASTA file and return sequences dictionary
    
    Args:
        filename: Path to FASTA file
        
    Returns:
        Dictionary of {sequence_name: sequence}
    """
    try:
        with open(filename, 'r') as f:
            content = f.read()
        return parse_fasta(content)
    except Exception as e:
        print(f"Error reading FASTA file {filename}: {e}")
        return {}

# =============================================================================
# SEQUENCE MANIPULATION & VALIDATION
# =============================================================================

def validate_sequence(sequence: str) -> Tuple[bool, str]:
    """
    Validate DNA sequence format and content
    
    Args:
        sequence: DNA sequence string
        
    Returns:
        (is_valid, error_message)
    """
    if not sequence:
        return False, "Empty sequence"
    
    if not isinstance(sequence, str):
        return False, "Sequence must be string"
    
    # Check for valid DNA characters
    valid_chars = set('ATGCRYSWKMBDHVN-')  # Include ambiguous bases
    invalid_chars = set(sequence.upper()) - valid_chars
    
    if invalid_chars:
        return False, f"Invalid characters found: {invalid_chars}"
    
    if len(sequence) < 10:
        return False, "Sequence too short (minimum 10 bp)"
    
    return True, "Valid sequence"

def reverse_complement(sequence: str) -> str:
    """
    Generate reverse complement of DNA sequence
    
    Args:
        sequence: DNA sequence string
        
    Returns:
        Reverse complement sequence
    """
    complement_map = {
        'A': 'T', 'T': 'A', 'G': 'C', 'C': 'G',
        'R': 'Y', 'Y': 'R', 'S': 'W', 'W': 'S',
        'K': 'M', 'M': 'K', 'B': 'V', 'V': 'B',
        'D': 'H', 'H': 'D', 'N': 'N', '-': '-'
    }
    
    return ''.join(complement_map.get(base.upper(), base) for base in reversed(sequence))

def gc_content(sequence: str) -> float:
    """
    Calculate GC content of sequence
    
    Args:
        sequence: DNA sequence string
        
    Returns:
        GC content as percentage (0-100)
    """
    if not sequence:
        return 0.0
    
    gc_count = sequence.upper().count('G') + sequence.upper().count('C')
    return (gc_count / len(sequence)) * 100

def at_content(sequence: str) -> float:
    """
    Calculate AT content of sequence
    
    Args:
        sequence: DNA sequence string
        
    Returns:
        AT content as percentage (0-100)
    """
    return 100.0 - gc_content(sequence)

def is_palindrome(sequence: str) -> bool:
    """
    Check if sequence is palindromic (same as reverse complement)
    
    Args:
        sequence: DNA sequence string
        
    Returns:
        True if palindromic
    """
    return sequence.upper() == reverse_complement(sequence).upper()

def calculate_tm(sequence: str) -> float:
    """
    Calculate melting temperature using nearest neighbor method (simplified)
    
    Args:
        sequence: DNA sequence string
        
    Returns:
        Melting temperature in Celsius
    """
    if len(sequence) < 2:
        return 0.0
    
    # Simplified Tm calculation
    gc = gc_content(sequence)
    length = len(sequence)
    
    if length <= 13:
        # For short sequences
        tm = (sequence.upper().count('A') + sequence.upper().count('T')) * 2 + \
             (sequence.upper().count('G') + sequence.upper().count('C')) * 4
    else:
        # For longer sequences
        tm = 64.9 + 41 * (gc / 100) - 650 / length
    
    return tm

def shuffle_sequence(sequence: str, seed: Optional[int] = None) -> str:
    """
    Generate shuffled version of sequence (preserving composition)
    
    Args:
        sequence: DNA sequence string
        seed: Random seed for reproducibility
        
    Returns:
        Shuffled sequence
    """
    import random
    if seed is not None:
        random.seed(seed)
    
    seq_list = list(sequence.upper())
    random.shuffle(seq_list)
    return ''.join(seq_list)

# =============================================================================
# STATISTICS & ANALYSIS FUNCTIONS
# =============================================================================

def get_basic_stats(sequence: str, motifs: Optional[List[Dict[str, Any]]] = None) -> Dict[str, Any]:
    """
    Calculate basic sequence statistics
    
    Args:
        sequence: DNA sequence string
        motifs: Optional list of detected motifs
        
    Returns:
        Dictionary of statistics
    """
    if not sequence:
        return {}
    
    seq = sequence.upper()
    length = len(seq)
    
    # Base composition
    base_counts = Counter(seq)
    
    stats = {
        'Length': length,
        'A': base_counts.get('A', 0),
        'T': base_counts.get('T', 0),
        'G': base_counts.get('G', 0),
        'C': base_counts.get('C', 0),
        'N': base_counts.get('N', 0),
        'GC%': round(gc_content(seq), 2),
        'AT%': round(at_content(seq), 2),
        'Tm': round(calculate_tm(seq), 1)
    }
    
    # Motif statistics if provided
    if motifs:
        stats.update(calculate_motif_statistics(motifs, length))
    
    return stats

def calculate_motif_statistics(motifs: List[Dict[str, Any]], sequence_length: int) -> Dict[str, Any]:
    """
    Calculate comprehensive motif statistics
    
    Args:
        motifs: List of motif dictionaries
        sequence_length: Length of analyzed sequence
        
    Returns:
        Dictionary of motif statistics
    """
    if not motifs:
        return {
            'Total_Motifs': 0,
            'Coverage%': 0.0,
            'Density': 0.0,
            'Classes_Detected': 0,
            'Subclasses_Detected': 0
        }
    
    # Count by class and subclass
    class_counts = Counter(m.get('Class', 'Unknown') for m in motifs)
    subclass_counts = Counter(m.get('Subclass', 'Unknown') for m in motifs)
    
    # Calculate coverage
    covered_positions = set()
    for motif in motifs:
        start = motif.get('Start', 0) - 1  # Convert to 0-based
        end = motif.get('End', 0)
        covered_positions.update(range(start, end))
    
    coverage_percent = (len(covered_positions) / sequence_length * 100) if sequence_length > 0 else 0
    density = len(motifs) / (sequence_length / 1000) if sequence_length > 0 else 0  # Motifs per kb
    
    stats = {
        'Total_Motifs': len(motifs),
        'Coverage%': round(coverage_percent, 2),
        'Density': round(density, 2),
        'Classes_Detected': len(class_counts),
        'Subclasses_Detected': len(subclass_counts),
        'Class_Distribution': dict(class_counts),
        'Subclass_Distribution': dict(subclass_counts)
    }
    
    # Score statistics
    scores = [m.get('Score', 0) for m in motifs if isinstance(m.get('Score'), (int, float))]
    if scores:
        stats.update({
            'Score_Mean': round(np.mean(scores), 3),
            'Score_Std': round(np.std(scores), 3),
            'Score_Min': round(min(scores), 3),
            'Score_Max': round(max(scores), 3)
        })
    
    # Length statistics
    lengths = [m.get('Length', 0) for m in motifs if isinstance(m.get('Length'), int)]
    if lengths:
        stats.update({
            'Length_Mean': round(np.mean(lengths), 1),
            'Length_Std': round(np.std(lengths), 1),
            'Length_Min': min(lengths),
            'Length_Max': max(lengths)
        })
    
    return stats

# =============================================================================
# FORMATTING & OUTPUT UTILITIES
# =============================================================================

def wrap(sequence: str, width: int = 80) -> str:
    """
    Wrap sequence to specified width
    
    Args:
        sequence: DNA sequence string
        width: Line width for wrapping
        
    Returns:
        Wrapped sequence string
    """
    if not sequence or width <= 0:
        return sequence
    
    return '\n'.join(sequence[i:i+width] for i in range(0, len(sequence), width))

def format_motif_rows(motifs: List[Dict[str, Any]]) -> List[List[str]]:
    """
    Format motifs for tabular display
    
    Args:
        motifs: List of motif dictionaries
        
    Returns:
        List of formatted rows
    """
    if not motifs:
        return []
    
    rows = []
    headers = ['Class', 'Subclass', 'Start', 'End', 'Length', 'Sequence', 'Score', 'Strand']
    
    for motif in motifs:
        row = []
        for header in headers:
            value = motif.get(header, 'N/A')
            if header == 'Sequence' and isinstance(value, str) and len(value) > 50:
                value = value[:47] + '...'
            elif header == 'Score' and isinstance(value, (int, float)):
                value = f"{value:.3f}"
            row.append(str(value))
        rows.append(row)
    
    return rows

def create_summary_table(sequences: Dict[str, str], results: Dict[str, List[Dict[str, Any]]]) -> pd.DataFrame:
    """
    Create summary table for multiple sequence analysis
    
    Args:
        sequences: Dictionary of {name: sequence}
        results: Dictionary of {name: motifs_list}
        
    Returns:
        Summary DataFrame
    """
    summary_data = []
    
    for name, sequence in sequences.items():
        motifs = results.get(name, [])
        stats = get_basic_stats(sequence, motifs)
        
        summary_data.append({
            'Sequence_Name': name,
            'Length_bp': stats.get('Length', 0),
            'GC_Content': stats.get('GC%', 0),
            'Total_Motifs': stats.get('Total_Motifs', 0),
            'Coverage_Percent': stats.get('Coverage%', 0),
            'Motif_Density': stats.get('Density', 0),
            'Classes_Detected': stats.get('Classes_Detected', 0),
            'Subclasses_Detected': stats.get('Subclasses_Detected', 0)
        })
    
    return pd.DataFrame(summary_data)

# =============================================================================
# DATA EXPORT FUNCTIONS
# =============================================================================

def export_to_bed(motifs: List[Dict[str, Any]], sequence_name: str = "sequence", 
                  filename: Optional[str] = None) -> str:
    """
    Export motifs to BED format
    
    Args:
        motifs: List of motif dictionaries
        sequence_name: Name of the sequence
        filename: Optional output filename
        
    Returns:
        BED format string
    """
    bed_lines = []
    bed_lines.append("track name=NBDScanner_motifs description=\"Non-B DNA motifs\" itemRgb=On")
    
    # Color mapping for different classes
    class_colors = {
        'Curved_DNA': '255,182,193',      # Light pink
        'Slipped_DNA': '255,218,185',     # Peach
        'Cruciform': '173,216,230',       # Light blue
        'R-Loop': '144,238,144',          # Light green
        'Triplex': '221,160,221',         # Plum
        'G-Quadruplex': '255,215,0',      # Gold
        'i-Motif': '255,165,0',           # Orange
        'Z-DNA': '138,43,226',            # Blue violet
        'A-philic_DNA': '230,230,250',    # Lavender
        'Hybrid': '192,192,192',          # Silver
        'Non-B_DNA_Clusters': '128,128,128'  # Gray
    }
    
    for motif in motifs:
        chrom = sequence_name
        start = max(0, motif.get('Start', 1) - 1)  # Convert to 0-based
        end = motif.get('End', start + 1)
        name = f"{motif.get('Class', 'Unknown')}_{motif.get('Subclass', 'Unknown')}"
        score = int(min(1000, max(0, motif.get('Score', 0) * 1000)))  # Scale to 0-1000
        strand = motif.get('Strand', '+')
        color = class_colors.get(motif.get('Class'), '128,128,128')
        
        bed_line = f"{chrom}\t{start}\t{end}\t{name}\t{score}\t{strand}\t{start}\t{end}\t{color}"
        bed_lines.append(bed_line)
    
    bed_content = '\n'.join(bed_lines)
    
    if filename:
        try:
            with open(filename, 'w') as f:
                f.write(bed_content)
        except Exception as e:
            print(f"Error writing BED file {filename}: {e}")
    
    return bed_content

def export_to_csv(motifs: List[Dict[str, Any]], filename: Optional[str] = None) -> str:
    """
    Export motifs to CSV format
    
    Args:
        motifs: List of motif dictionaries
        filename: Optional output filename
        
    Returns:
        CSV format string
    """
    if not motifs:
        return "No motifs to export"
    
    # Get all unique keys from motifs
    all_keys = set()
    for motif in motifs:
        all_keys.update(motif.keys())
    
    # Standard column order
    standard_columns = ['ID', 'Sequence_Name', 'Class', 'Subclass', 'Start', 'End', 
                       'Length', 'Sequence', 'Score', 'Strand', 'Method']
    
    # Arrange columns with standard ones first
    columns = [col for col in standard_columns if col in all_keys]
    columns.extend([col for col in sorted(all_keys) if col not in standard_columns])
    
    output = StringIO()
    writer = csv.DictWriter(output, fieldnames=columns)
    writer.writeheader()
    
    for motif in motifs:
        # Ensure all columns have values
        row = {col: motif.get(col, '') for col in columns}
        writer.writerow(row)
    
    csv_content = output.getvalue()
    output.close()
    
    if filename:
        try:
            with open(filename, 'w', newline='') as f:
                f.write(csv_content)
        except Exception as e:
            print(f"Error writing CSV file {filename}: {e}")
    
    return csv_content

def export_to_json(motifs: List[Dict[str, Any]], filename: Optional[str] = None, 
                   pretty: bool = True) -> str:
    """
    Export motifs to JSON format
    
    Args:
        motifs: List of motif dictionaries
        filename: Optional output filename
        pretty: Whether to format JSON prettily
        
    Returns:
        JSON format string
    """
    json_data = {
        'version': '2024.1',
        'analysis_type': 'NBDScanner_Non-B_DNA_Analysis',
        'total_motifs': len(motifs),
        'motifs': motifs
    }
    
    if pretty:
        json_content = json.dumps(json_data, indent=2, ensure_ascii=False)
    else:
        json_content = json.dumps(json_data, ensure_ascii=False)
    
    if filename:
        try:
            with open(filename, 'w') as f:
                f.write(json_content)
        except Exception as e:
            print(f"Error writing JSON file {filename}: {e}")
    
    return json_content

def export_to_gff3(motifs: List[Dict[str, Any]], sequence_name: str = "sequence", 
                   filename: Optional[str] = None) -> str:
    """
    Export motifs to GFF3 format
    
    Args:
        motifs: List of motif dictionaries
        sequence_name: Name of the sequence
        filename: Optional output filename
        
    Returns:
        GFF3 format string
    """
    gff_lines = []
    gff_lines.append("##gff-version 3")
    gff_lines.append(f"##sequence-region {sequence_name} 1 {len(sequence_name)}")
    
    for i, motif in enumerate(motifs, 1):
        seqid = sequence_name
        source = "NBDScanner"
        feature_type = "Non_B_DNA_motif"
        start = motif.get('Start', 1)
        end = motif.get('End', start)
        score = motif.get('Score', '.')
        strand = motif.get('Strand', '+')
        phase = "."
        
        # Attributes
        attributes = [
            f"ID=motif_{i}",
            f"Name={motif.get('Class', 'Unknown')}_{motif.get('Subclass', 'Unknown')}",
            f"motif_class={motif.get('Class', 'Unknown')}",
            f"motif_subclass={motif.get('Subclass', 'Unknown')}",
            f"length={motif.get('Length', 0)}",
            f"method={motif.get('Method', 'NBDScanner')}"
        ]
        
        attributes_str = ';'.join(attributes)
        
        gff_line = f"{seqid}\t{source}\t{feature_type}\t{start}\t{end}\t{score}\t{strand}\t{phase}\t{attributes_str}"
        gff_lines.append(gff_line)
    
    gff_content = '\n'.join(gff_lines)
    
    if filename:
        try:
            with open(filename, 'w') as f:
                f.write(gff_content)
        except Exception as e:
            print(f"Error writing GFF3 file {filename}: {e}")
    
    return gff_content

# =============================================================================
# QUALITY CONTROL & FILTERING
# =============================================================================

def quality_check_motifs(motifs: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    Perform quality checks on detected motifs
    
    Args:
        motifs: List of motif dictionaries
        
    Returns:
        Quality check report
    """
    if not motifs:
        return {'status': 'No motifs to check', 'passed': True, 'issues': []}
    
    issues = []
    
    # Check for required fields
    required_fields = ['Class', 'Subclass', 'Start', 'End', 'Sequence']
    for i, motif in enumerate(motifs):
        missing_fields = [field for field in required_fields if field not in motif]
        if missing_fields:
            issues.append(f"Motif {i+1}: Missing fields {missing_fields}")
    
    # Check coordinate consistency
    for i, motif in enumerate(motifs):
        start = motif.get('Start')
        end = motif.get('End')
        length = motif.get('Length')
        sequence = motif.get('Sequence', '')
        
        if start and end and start >= end:
            issues.append(f"Motif {i+1}: Invalid coordinates (start >= end)")
        
        if start and end and length and (end - start + 1) != length:
            issues.append(f"Motif {i+1}: Length inconsistent with coordinates")
        
        if sequence and length and len(sequence) != length:
            issues.append(f"Motif {i+1}: Sequence length doesn't match reported length")
    
    # Check for overlaps within same class
    class_groups = defaultdict(list)
    for motif in motifs:
        class_groups[motif.get('Class')].append(motif)
    
    for class_name, class_motifs in class_groups.items():
        for i, motif1 in enumerate(class_motifs):
            for motif2 in class_motifs[i+1:]:
                if (motif1.get('Start', 0) < motif2.get('End', 0) and 
                    motif2.get('Start', 0) < motif1.get('End', 0)):
                    issues.append(f"Overlapping motifs in class {class_name}")
                    break
    
    report = {
        'total_motifs': len(motifs),
        'issues_found': len(issues),
        'passed': len(issues) == 0,
        'issues': issues[:10],  # Limit to first 10 issues
        'status': 'PASSED' if len(issues) == 0 else f'FAILED ({len(issues)} issues)'
    }
    
    return report

def filter_motifs_by_score(motifs: List[Dict[str, Any]], min_score: float = 0.0) -> List[Dict[str, Any]]:
    """
    Filter motifs by minimum score threshold
    
    Args:
        motifs: List of motif dictionaries
        min_score: Minimum score threshold
        
    Returns:
        Filtered motifs list
    """
    return [m for m in motifs if m.get('Score', 0) >= min_score]

def filter_motifs_by_length(motifs: List[Dict[str, Any]], 
                           min_length: int = 0, max_length: int = float('inf')) -> List[Dict[str, Any]]:
    """
    Filter motifs by length range
    
    Args:
        motifs: List of motif dictionaries
        min_length: Minimum length
        max_length: Maximum length
        
    Returns:
        Filtered motifs list
    """
    return [m for m in motifs if min_length <= m.get('Length', 0) <= max_length]

def filter_motifs_by_class(motifs: List[Dict[str, Any]], 
                          allowed_classes: List[str]) -> List[Dict[str, Any]]:
    """
    Filter motifs by allowed classes
    
    Args:
        motifs: List of motif dictionaries
        allowed_classes: List of allowed class names
        
    Returns:
        Filtered motifs list
    """
    return [m for m in motifs if m.get('Class') in allowed_classes]

# =============================================================================
# CROSS-DETECTOR OVERLAP RESOLUTION (Hyperscan Integration Pattern)
# =============================================================================

def resolve_cross_class_overlaps(motifs: List[Dict[str, Any]], 
                                 mode: str = 'strict') -> List[Dict[str, Any]]:
    """
    Resolve overlaps across different motif classes using deterministic selection.
    
    Implements the overlap resolution strategy from the Hyperscan integration plan:
    - Option A (strict): Select highest normalized score among overlapping regions
    - Option B (hybrid): Keep both but mark as Hybrid (handled by detector pipeline)
    
    Algorithm:
    1. Sort candidates by score (descending), then by length (descending)
    2. Greedily select highest-scoring non-overlapping motifs
    3. This ensures deterministic, reproducible output
    
    Args:
        motifs: List of motif dictionaries from multiple detectors
        mode: 'strict' for non-overlapping selection, 'hybrid' for hybrid marking
        
    Returns:
        List of resolved motifs (non-overlapping if mode='strict')
        
    References:
        - Hyperscan integration pattern for Non-B DNA detection
        - Score-aware greedy selection (similar to Cruciform._remove_overlaps)
    """
    if not motifs:
        return []
    
    if mode == 'hybrid':
        # Return all motifs - hybrid detection happens in the scanner pipeline
        return motifs
    
    # Mode 'strict': Select highest-scoring non-overlapping set
    # Sort by score (descending), then by length (descending) for tie-breaking
    sorted_motifs = sorted(motifs, 
                          key=lambda x: (-x.get('Score', 0), 
                                       -(x.get('End', 0) - x.get('Start', 0))))
    
    selected = []
    
    for candidate in sorted_motifs:
        # Check if this candidate overlaps with any already selected motif
        overlaps = False
        cand_start = candidate.get('Start', 0)
        cand_end = candidate.get('End', 0)
        
        for selected_motif in selected:
            sel_start = selected_motif.get('Start', 0)
            sel_end = selected_motif.get('End', 0)
            
            # Check for overlap: two regions overlap if neither is completely before the other
            if not (cand_end <= sel_start or cand_start >= sel_end):
                overlaps = True
                break
        
        if not overlaps:
            selected.append(candidate)
    
    # Sort by start position for final output
    selected.sort(key=lambda x: x.get('Start', 0))
    
    return selected

def merge_detector_results(detector_results: Dict[str, List[Dict[str, Any]]],
                          overlap_mode: str = 'strict') -> List[Dict[str, Any]]:
    """
    Merge results from multiple detectors with overlap resolution.
    
    This function implements the complete Hyperscan integration pattern:
    1. Collect results from all detectors (already scored and filtered)
    2. Resolve cross-class overlaps according to specified mode
    3. Return unified, non-redundant motif list
    
    Args:
        detector_results: Dictionary mapping detector_name -> list of motifs
        overlap_mode: 'strict' for non-overlapping, 'hybrid' for overlap annotation
        
    Returns:
        Merged list of motifs with overlaps resolved
        
    Example:
        >>> results = {
        ...     'a_philic': [motif1, motif2],
        ...     'z_dna': [motif3, motif4],
        ...     'g_quadruplex': [motif5]
        ... }
        >>> merged = merge_detector_results(results, overlap_mode='strict')
    """
    # Flatten all detector results into single list
    all_motifs = []
    for detector_name, motifs in detector_results.items():
        # Add detector source to each motif for tracking
        for motif in motifs:
            if 'Detector' not in motif:
                motif['Detector'] = detector_name
            all_motifs.append(motif)
    
    # Resolve overlaps according to mode
    resolved = resolve_cross_class_overlaps(all_motifs, mode=overlap_mode)
    
    return resolved

# =============================================================================
# TESTING & EXAMPLES
# =============================================================================

def test_utilities():
    """Test utility functions with example data"""
    print("Testing NBDScanner utilities...")
    
    # Test sequence validation
    test_sequences = [
        "ATGCATGCATGC",     # Valid
        "ATGCXYZ",          # Invalid characters
        "ATG",              # Too short
        "",                 # Empty
    ]
    
    print("\nSequence validation tests:")
    for seq in test_sequences:
        valid, msg = validate_sequence(seq)
        print(f"  '{seq[:20]}': {'VALID' if valid else 'INVALID'} - {msg}")
    
    # Test basic statistics
    test_seq = "GGGTTAGGGTTAGGGTTAGGGAAAAATTTTCGCGCGCGCG"
    stats = get_basic_stats(test_seq)
    print(f"\nBasic stats for test sequence:")
    for key, value in stats.items():
        print(f"  {key}: {value}")
    
    # Test FASTA parsing
    fasta_content = """>test_sequence
GGGTTAGGGTTAGGGTTAGGG
>another_sequence
AAAAATTTTCCCCGGGG"""
    
    sequences = parse_fasta(fasta_content)
    print(f"\nFASTA parsing test: {len(sequences)} sequences parsed")
    for name, seq in sequences.items():
        print(f"  {name}: {seq}")
    
    # Test motif formatting
    example_motifs = [
        {
            'Class': 'G-Quadruplex',
            'Subclass': 'Canonical G4',
            'Start': 1,
            'End': 21,
            'Length': 21,
            'Sequence': 'GGGTTAGGGTTAGGGTTAGGG',
            'Score': 0.857,
            'Strand': '+'
        }
    ]
    
    quality_report = quality_check_motifs(example_motifs)
    print(f"\nQuality check: {quality_report['status']}")
    
    print("✓ All utility tests completed")

if __name__ == "__main__":
    test_utilities()